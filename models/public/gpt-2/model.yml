# Copyright (c) 2021 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

description: >-
  The "gpt-2" model is a one of Generative Pre-trained Transformer (GPT) model family,
  pre-trained on a very large corpus of English data in a self-supervised fashion.
  The GPT architecture implements a deep neural network, specifically a transformer
  model, which uses attention in place of previous recurrence- and convolution-based
  architectures. Attention mechanisms allow the model to selectively focus on segments
  of input text it predicts to be the most relevant. GPT-2 is trained with a simple
  objective: predict the next word, given all of the previous words within some text.

  More details provided in the paper <https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>,
  repository <https://github.com/huggingface/transformers> and model card <https://huggingface.co/gpt2>.
task_type: text_prediction
files:
  - name: transformers-4.9.1-py3-none-any.whl
    size: 2586799
    sha256: 86f3c46efecf114c6886d361c1d6cca14738f0e9d1effadb1e9252770cba55a0
    source: https://files.pythonhosted.org/packages/38/39/5238c128cef0f4405c568b1e8c9c7423202109dcbb1622333918d8de1cd3/transformers-4.9.1-py3-none-any.whl
  - name: gpt2/pytorch_model.bin
    size: 548118077
    sha256: 7c5d3f4b8b76583b422fcb9189ad6c89d5d97a094541ce8932dce3ecabde1421
    source: https://huggingface.co/gpt2/resolve/main/pytorch_model.bin
  - name: gpt2/config.json
    size: 665
    sha256: 0daed7749b4f02b8f76240d5444551d7b08712dab4d0adb8239c56ba823bb7b4
    source: https://huggingface.co/gpt2/resolve/main/config.json
  - name: gpt2/vocab.json
    size: 1042301
    sha256: 196139668be63f3b5d6574427317ae82f612a97c5d1cdaf36ed2256dbf636783
    source: https://huggingface.co/gpt2/resolve/main/vocab.json
  - name: gpt2/merges.txt
    size: 456318
    sha256: 1ce1664773c50f3e0cc8842619a93edc4624525b728b188a9e0be33b7726adc5
    source: https://huggingface.co/gpt2/resolve/main/merges.txt
  - name: packaging-21.0-py3-none-any.whl
    size: 40357
    sha256: c86254f9220d55e31cc94d69bade760f0847da8000def4dfe1c6b872fd14ff14
    source: https://files.pythonhosted.org/packages/3c/77/e2362b676dc5008d81be423070dd9577fa03be5da2ba1105811900fda546/packaging-21.0-py3-none-any.whl
postprocessing:
  - $type: unpack_archive
    format: zip
    file: transformers-4.9.1-py3-none-any.whl
  - $type: unpack_archive
    format: zip
    file: packaging-21.0-py3-none-any.whl
  - $type: regex_replace
    file: transformers/__init__.py
    pattern: 'from . import dependency_versions_check'
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/file_utils.py
    pattern: 'from tqdm.auto import tqdm'
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/file_utils.py
    pattern: from filelock import FileLock
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/data/datasets/glue.py
    pattern: from filelock import FileLock
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/data/datasets/squad.py
    pattern: from filelock import FileLock
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/data/datasets/language_modeling.py
    pattern: from filelock import FileLock
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/file_utils.py
    pattern: from huggingface_hub import HfApi, HfFolder, Repository
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/file_utils.py
    pattern: Repository
    replacement: None
  - $type: regex_replace
    file: transformers/modelcard.py
    pattern: from huggingface_hub import HfApi
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/deepspeed.py
    pattern: from .dependency_versions_check import dep_version_check
    replacement: '# \g<0>'
  - $type: regex_replace
    file: transformers/trainer.py
    pattern: from .dependency_versions_check import dep_version_check
    replacement: '# \g<0>'
conversion_to_onnx_args:
  - --model-path=$dl_dir
  - --model-path=$config_dir
  - --model-name=create_model
  - --import-module=model
  - --model-param=model_dir=r"$dl_dir/gpt2"
  - --input-names=input
  - --output-names=output
  - --input-shapes=[1,1024]
  - --output-file=$conv_dir/gpt-2.onnx
  - --inputs-dtype=long
model_optimizer_args:
  - --input_shape=[1,1024]
  - --input=input
  - --input_model=$dl_dir/gpt-2.onnx
  - --output=output
framework: pytorch
license: https://raw.githubusercontent.com/huggingface/transformers/master/LICENSE
