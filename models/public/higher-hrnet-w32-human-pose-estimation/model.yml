# Copyright (c) 2022-2023 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

description: >-
  The "HigherHRNet-W32" model is one of the HigherHRNet <https://arxiv.org/pdf/1908.10357>.
  "HigherHRNet" is a novel bottom-up human pose estimation method for learning scale-aware
  representations using high-resolution feature pyramids. The network uses HRNet as
  backbone, followed by one or more deconvolution modules to generate multi-resolution
  and high-resolution heatmaps. For every person in an image, the network detects
  a human pose: a body skeleton consisting of keypoints and connections between them.
  The pose may contain up to 17 keypoints: ears, eyes, nose, shoulders, elbows, wrists,
  hips, knees, and ankles. This is PyTorch* implementation pre-trained on COCO dataset.
  For details about implementation of model, check out the HigherHRNet: Scale-Aware
  Representation Learning for Bottom-Up Human Pose Estimation <https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation>
  repository.
task_type: human_pose_estimation
files:
  - name: models/__init__.py
    size: 412
    checksum: fe099091a8545442577baf0886860846c2485fb991ce358e8632eeb10cc6a3827c54a64b1f0150c1fcb12ebeb103f625
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/lib/models/__init__.py
  - name: models/pose_higher_hrnet.py
    size: 21313
    checksum: 2772ce487f9c3407b814e67c6b017ea107caafc23a1cb64f9b8f855239d16dbd2ad3030825e04c29ba81b8a31f80e034
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/lib/models/pose_higher_hrnet.py
  - name: config/__init__.py
    size: 370
    checksum: b1c2faece903938d66f5969981daee79fb2322c9e9998189ab92ede8e3a2c08a9b2d2c0e257e118938e5bf1c4cf7bffd
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/lib/config/__init__.py
  - name: config/default.py
    size: 6075
    checksum: ad03d7165998982d22b8d7cdc3604f3b0bbbcebe8de4f1ff8d6897e8e297c3bffa0b83099c3ff1290407da5f29434220
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/lib/config/default.py
  - name: config/models.py
    size: 2518
    checksum: 2ae3fc62047e9838e8d1fbc59d2d127979f55a0672a5a2f43088447dabaa5f3d4039eb2f196c59f7f38d4e0d853ff58d
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/lib/config/models.py
  - name: experiments/higher_hrnet.yaml
    size: 2326
    checksum: 1a8c79097c602a777ec1182a081258562844bfe98f9b4aadecbcb56e25f2f3ffed9db285121f9d819514eee4319228d4
    source: https://raw.githubusercontent.com/HRNet/HigherHRNet-Human-Pose-Estimation/f97496fdaa5365ee33d44c7872da21375fb1a39c/experiments/coco/higher_hrnet/w32_512_adam_lr1e-3.yaml
  - name: ckpt/pose_higher_hrnet_w32_512.pth
    size: 115172215
    checksum: d344fcc4e426308139397bd708be1c59500e038a540ea0979b6a10556b5f3530701d86c0cab276fc5710419f91879e3d
    original_source:
      $type: google_drive
      id: 1V9Iz0ZYy9m8VeaspfKECDW0NKlGsYmO1
    source: https://storage.openvinotoolkit.org/repositories/open_model_zoo/public/2022.1/higher-hrnet-w32-human-pose-estimation/ckpt/pose_higher_hrnet_w32_512.pth
conversion_to_onnx_args:
  - --model-path=$config_dir
  - --model-path=$dl_dir
  - --model-name=get_net
  - --import-module=model
  - --model-param=file_config=r"$dl_dir/experiments/higher_hrnet.yaml"
  - --model-param=weights=r"$dl_dir/ckpt/pose_higher_hrnet_w32_512.pth"
  - --input-shape=1,3,512,512
  - --input-names=image
  - '--conversion-param=dynamic_axes={"image": {2: "height", 3: "width"}}'
  - --output-names=embeddings,heatmaps
  - --output-file=$conv_dir/higher-hrnet-w32-human-pose-estimation.onnx
input_info:
  - name: image
    shape: [1, 3, 512, 512]
    layout: NCHW
model_optimizer_args:
  - --reverse_input_channels
  - --mean_values=image[123.675,116.28,103.53]
  - --scale_values=image[58.395,57.12,57.375]
  - --output=embeddings,heatmaps
  - --input_model=$conv_dir/higher-hrnet-w32-human-pose-estimation.onnx
framework: pytorch
license: https://raw.githubusercontent.com/CSAILVision/semantic-segmentation-pytorch/9aff40de31ee4b21f18514d31e5d6e4ba056924d/LICENSE
