models:
  - name: googlenet-v3-pytorch

    launchers:
      - framework: onnx_runtime
        model: public/googlenet-v3-pytorch/googlenet-v3.onnx
        adapter: classification
        inputs:
          - name: data
            type: INPUT
            shape: 1,3,224,224

    datasets:
      - name: imagenet_1000_classes
        # images read with Pillow
        reader: pillow_imread

        preprocessing:
          - type: resize
            size: 320
            aspect_ratio_scale: greater
            use_pillow: true
            interpolation: BILINEAR
          - type: crop
            size: 299
            use_pillow: true
          - type: normalization
            mean: 127.5
            std: 127.5

            # Using accuracy metric, achieved result of public model - 77.45% and 93.56% (top 1 and top 5 respectively)

  - name: googlenet-v3-pytorch

    # list of launchers
    launchers:
      - framework: dlsdk
        tags:
          - FP32
        model:   public/googlenet-v3-pytorch/FP32/googlenet-v3-pytorch.xml
        weights: public/googlenet-v3-pytorch/FP32/googlenet-v3-pytorch.bin
        adapter: classification

      - framework: dlsdk
        tags:
          - FP16
        model:   public/googlenet-v3-pytorch/FP16/googlenet-v3-pytorch.xml
        weights: public/googlenet-v3-pytorch/FP16/googlenet-v3-pytorch.bin
        adapter: classification

      - framework: dlsdk
        tags:
          - FP32-INT8
        model:   public/googlenet-v3-pytorch/FP32-INT8/googlenet-v3-pytorch.xml
        weights: public/googlenet-v3-pytorch/FP32-INT8/googlenet-v3-pytorch.bin
        adapter: classification

      - framework: dlsdk
        tags:
          - FP16-INT8
        model:   public/googlenet-v3-pytorch/FP16-INT8/googlenet-v3-pytorch.xml
        weights: public/googlenet-v3-pytorch/FP16-INT8/googlenet-v3-pytorch.bin
        adapter: classification

    datasets:
      - name: imagenet_1000_classes
        # images read with Pillow
        reader: pillow_imread

        preprocessing:
          - type: resize
            size: 320
            aspect_ratio_scale: greater
            use_pillow: true
            interpolation: BILINEAR
          - type: crop
            size: 299
            use_pillow: true
          # Image channels must be swapped, because "pillow_imread" reads in RGB, but converted model expect BGR
          - type: rgb_to_bgr

            # Using accuracy metric, achieved result of public model - 77.45% and 93.56% (top 1 and top 5 respectively)
