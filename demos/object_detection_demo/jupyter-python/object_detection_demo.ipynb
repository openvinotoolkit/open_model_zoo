{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overall-running",
   "metadata": {},
   "source": [
    "# Open Model Zoo Object Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-simon",
   "metadata": {},
   "source": [
    "This demo showcases Object Detection on Open Model Zoo models with Async API.\n",
    "\n",
    "Async API usage can improve the overall frame-rate of the application, because rather than wait for inference to complete, the app can continue doing things on the host, while accelerator is busy.\n",
    "\n",
    "Other demo objectives are:\n",
    "\n",
    "* Video as input support via OpenCV\\*\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Comparison of different Open Model Zoo detection models\n",
    "\n",
    "See the [Python Object Detection Async Demo](../python/) for more details about the Async API, and the [Optimization Guide](https://docs.openvinotoolkit.org/latest/_docs_optimization_guide_dldt_optimization_guide.html) for more information on optimizing models.\n",
    "\n",
    "Note that the frame rates shown in this demo are an indication and not a true measure of performance of a model. Use the [OpenVINO Benchmark Tool](https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/tools/benchmark_tool) to get a better measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-spank",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, clear_output\n",
    "from ipywidgets import Layout, fixed, interact, interact_manual\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "from detection_utils import ColorPalette, download_video, draw_detections, get_model, put_highlighted_text\n",
    "\n",
    "# Add the Open Model Zoo common python folder to the path, to import the pipelines module.\n",
    "open_model_zoo_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(os.curdir))))\n",
    "sys.path.append(os.path.join(open_model_zoo_path, \"demos\", \"common\", \"python\"))\n",
    "\n",
    "from pipelines import AsyncPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-direction",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Set the file and directory paths. The default settings expect that the models are located in `open_model_zoo_models` in your `$HOME` directory, typically `C:\\Users\\username` or `/home/username`. You can change this by setting the `base_model_dir` variable to another directory. Set `models_file` to `models-all.lst` to use all supported models, instead of a subset. This wil increase model download and conversion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "## File settings\n",
    "# Directory that contains the Open Model Zoo models. It has subdirectories \"intel\" and \"public\"\n",
    "base_model_dir = os.path.expanduser(\"~/open_model_zoo_models\")\n",
    "# Directory for Open Model Zoo cache files. Caching speeds up subsequent downloads.\n",
    "omz_cache_dir = os.path.expanduser(\"~/open_model_zoo_cache\")\n",
    "models_file = \"models-subset.lst\"  # models-subset.lst contains a subset of supported models.\n",
    "# models_file = 'models.lst'  # models.lst contains all supported models.\n",
    "\n",
    "## Model settings\n",
    "DEVICE = \"CPU\"\n",
    "PRECISION = \"FP16\"\n",
    "\n",
    "## Demo settings\n",
    "DOWNLOAD_MODELS = True  # Use Model Downloader to download models from Open Model Zoo\n",
    "JUMP_FRAMES = 10  # Read every n-th frame of the input video\n",
    "PROB_THRESHOLD = 0.5  # The probability threshold for detection predictions\n",
    "DEFAULT_NUM_THREADS = 3  # Default number of threads to use for inference\n",
    "DEFAULT_NUM_STREAMS = 3  # Default number of streams to use for inference\n",
    "DEFAULT_NUM_REQUESTS = 4  # Default maximum number of requests to use for inference\n",
    "\n",
    "## Visualization settings\n",
    "PALETTE = ColorPalette(100)\n",
    "FONT_SCALE = 1\n",
    "THICKNESS = 2\n",
    "\n",
    "# The settings below are only required if you want to use the Model Converter to convert models to OpenVINO IR format.\n",
    "# You can use this demo with models that are already downloaded in IR format, so use of the model optimizer is optional.\n",
    "\n",
    "# The path to the Model Optimizer is required if models need to be converted to IR. The paths below should work for default installations of\n",
    "# the Intel Distribution of OpenVINO Toolkit https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html\n",
    "# Adjust them if you installed OpenVINO in a different location.\n",
    "# Note that you also need to install the Model Optimizer prerequisites. See the documentation for your OS at\n",
    "# https://docs.openvinotoolkit.org/latest/installation_guides.html\n",
    "\n",
    "CONVERT_MODELS = False  # Set to True to use public Open Model Zoo models and convert them with the Model Optimizer.\n",
    "if CONVERT_MODELS:\n",
    "    if sys.platform.startswith(\"win\"):  # Windows\n",
    "        model_optimizer_path = r\"C:\\Program Files (x86)\\intel\\openvino_2021\\deployment_tools\\model_optimizer\\mo.py\"\n",
    "    else:  # Linux/MacOS\n",
    "        model_optimizer_path = \"/opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-valuable",
   "metadata": {},
   "source": [
    "## Download Models and convert them to IR format\n",
    "\n",
    "The [Model Downloader](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/downloader/README.md) downloads models from the Open Model Zoo. Models that are not in OpenVINO IR format are converted to this format by the Model Converter. \n",
    "\n",
    "A subset of [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models that are compatible with this demo are listed in the file `models_file` (default \"models.lst\") in the same folder as this notebook. By default these models are downloaded, with the `--list=models_file` argument for the Model Downloader. You can choose to download a specific model by using `--name=model_name` instead of `--list=models.lst`. If you already have downloaded Open Zoo Models, you can set the `base_model_dir` variable in the *Settings* cell to the folder that contains your models (this should be a folder with subfolders `intel` and `public`) and set `DOWNLOAD_MODELS` to `False`.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\"><i>\n",
    "<b>Note: </b>It will take a while to download and convert all the models. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_MODELS:\n",
    "    downloader_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"downloader.py\")\n",
    "    download_result = subprocess.run(\n",
    "        [\n",
    "            sys.executable,  # the path to the Python executable\n",
    "            downloader_command,\n",
    "            \"--output_dir\",\n",
    "            base_model_dir,\n",
    "            \"--cache_dir\",\n",
    "            omz_cache_dir,\n",
    "            \"--precision\",\n",
    "            PRECISION,\n",
    "            \"--list\",\n",
    "            models_file,\n",
    "            \"--jobs\",\n",
    "            \"4\",\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        shell=False,\n",
    "        text=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_result.returncode == 0:\n",
    "    print(\n",
    "        \"Downloading models succeeded. You can set `DOWNLOAD_MODELS=False` to save some time when you run this notebook again.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading models failed. The error message is: {download_result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the models that are not in IR format to IR\n",
    "if CONVERT_MODELS:\n",
    "    converter_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"converter.py\")\n",
    "    converter_result = subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            converter_command,\n",
    "            \"--download_dir\",\n",
    "            base_model_dir,\n",
    "            \"--list\",\n",
    "            models_file,\n",
    "            \"--precisions\",\n",
    "            PRECISION,\n",
    "            \"--mo\",\n",
    "            model_optimizer_path,\n",
    "            \"--jobs\",\n",
    "            \"4\",\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        shell=False,\n",
    "        text=True,\n",
    "    )\n",
    "    if converter_result.returncode == 0:\n",
    "        print(\n",
    "            \"Converting models succeeded. Set CONVERT_MODELS to False to save time when you run this notebook again.\"\n",
    "        )\n",
    "    else:\n",
    "        error_message = \"Not all models were converted succesfully. Check `converter_result.stderr` for more details. \"\n",
    "        if \"No module named\" in converter_result.stderr:\n",
    "            error_message += \"Make sure that the Model Optimizer requirements are installed before running the Model Converter. See the <a href='https://docs.openvinotoolkit.org/latest/installation_guides.html'>OpenVINO installation guide</a> for more information\"\n",
    "        display(HTML(f'<div class=\"alert alert-warning\" style=\"color:black\"><i><b>Warning: </b>{error_message}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-thesaurus",
   "metadata": {},
   "source": [
    "### Get model info\n",
    "\n",
    "The Info Dumper returns information for the Open Model Zoo models. It returns a list of dictionaries with the model name, description, framework, license url, precisions, task type, and the subdirectory for the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"info_dumper.py\")\n",
    "info_result = subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        info_command,\n",
    "        \"--list\",\n",
    "        models_file,\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    capture_output=False,\n",
    "    shell=False,\n",
    "    text=True,\n",
    ")\n",
    "info = json.loads(info_result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-short",
   "metadata": {},
   "source": [
    "Make a list of models that will be shown as options. By default, only Intel models will be shown. Change this by uncommenting the second line to use all models. You need to make sure that the models are in IR format. This can be done by setting `CONVERT_MODELS` to `True` and running all cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [model[\"name\"] for model in info if \"intel\" in model[\"subdirectory\"]]\n",
    "# model_names = [model[\"name\"] for model in info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of the information that the Info Dumper returns\n",
    "info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-theater",
   "metadata": {},
   "source": [
    "The `models_file` file, by default \"models.lst\", lists models that are supported by this demo, sorted by architecture. The model names can contain wildcard. For example, `face-detection-????` means that the demo supports all models with a name that starts with `face-detection-` followed by four digits. \n",
    "\n",
    "We create a `model_architectures` dictionary that maps the model names given by the Info Dumper, to an architecture given by `models_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architectures = {}\n",
    "modellist = open(models_file).read().splitlines()\n",
    "\n",
    "for line in modellist[1:]:\n",
    "    if line.startswith(\"# For\"):\n",
    "        _, architecture = line.split(\"=\")\n",
    "    else:\n",
    "        model_architectures[line] = architecture\n",
    "        for modelname in model_names:\n",
    "            modelpattern = re.search(line.replace(\"?\", \"[0-9]\"), modelname)\n",
    "            if modelpattern:\n",
    "                model_architectures[modelpattern.group(0)] = architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-migration",
   "metadata": {},
   "source": [
    "## Create inference functions\n",
    "\n",
    "The `do_inference_on_video` function performs the inference of a model on a specific video. The helper function `process_results` add the time to the result from the pipeline, so that the inference speed can be computed. The function opens the video file given by `input_filename` with OpenCV's `VideoCapture`. It reads the frames sequentially, `jump_frames` frames at a time. If `jump_frames=1` all frames will be read. By default `jump_frames=10` which means that every tenth frame will be read. While there are new frames, the code:\n",
    "\n",
    "* Checks if there are results from the pipeline. If there are, it records the time, and adds the result to the list of results\n",
    "* Checks if the pipeline is ready. If it is, it sees if there is a new frame. \n",
    "  * If there is a new frame (we have not reached the end of the video), the frame is read, and sent to the detector pipeline for inference. \n",
    "  * If there are no more frames, the video is closed\n",
    "\n",
    "At the end of the function, we wait until the detector is finished, and add the final results to the list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference_on_video(detector_pipeline, input_filename, jump_frames):\n",
    "    \"\"\"\n",
    "    Perform asynchronous inference on a given detector_pipeline with video from input_filename.\n",
    "    `jump_frames` determines how many frames will be read from the video. If jump_frames=N, every Nth frame\n",
    "    of the video will be read.\n",
    "    \"\"\"\n",
    "    resultlist = []\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "    overall_start_time = perf_counter()\n",
    "\n",
    "    def process_results(results):\n",
    "        \"\"\"Helper function to add inference time to results\"\"\"\n",
    "        outputs, meta = results\n",
    "        meta[\"end_time\"] = perf_counter()\n",
    "        meta[\"overall_start_time\"] = overall_start_time\n",
    "        return outputs, meta\n",
    "\n",
    "    cap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, next_frame_id)\n",
    "        if detector_pipeline.callback_exceptions:\n",
    "            raise detector_pipeline.callback_exceptions[0]\n",
    "\n",
    "        # Process all completed requests\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "            next_frame_id_to_show += jump_frames\n",
    "\n",
    "        if detector_pipeline.is_ready():\n",
    "            # Get new image/frame\n",
    "            start_time = perf_counter()\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            # Submit for inference\n",
    "            detector_pipeline.submit_data(frame, next_frame_id, {\"frame\": frame, \"start_time\": start_time})\n",
    "            next_frame_id += jump_frames\n",
    "\n",
    "        else:\n",
    "            # Wait for empty request\n",
    "            detector_pipeline.await_any()\n",
    "\n",
    "    detector_pipeline.await_all()\n",
    "\n",
    "    while detector_pipeline.has_completed_request():\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "            next_frame_id_to_show += jump_frames\n",
    "\n",
    "    return resultlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-formation",
   "metadata": {},
   "source": [
    "The `make_result_frames` function takes the output of the `do_inference_on_video` function and returns a list of videoframes with detection boxes drawn on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_frames(inference_result, has_landmarks):\n",
    "    \"\"\" \"\n",
    "    Draw boxes on frames from inference results and return the list of frames.\n",
    "    \"\"\"\n",
    "    framelist = list()\n",
    "\n",
    "    for i, (objects, meta) in enumerate(inference_result):\n",
    "        start_time = meta[\"start_time\"]\n",
    "        overall_start_time = meta[\"overall_start_time\"]\n",
    "        end_time = meta[\"end_time\"]\n",
    "        latency = (end_time - start_time) * 1000\n",
    "        fps = (i + 1) / (end_time - overall_start_time)\n",
    "\n",
    "        frame = meta[\"frame\"]\n",
    "        frame = draw_detections(\n",
    "            frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),\n",
    "            detections=objects,\n",
    "            palette=PALETTE,\n",
    "            labels=None,\n",
    "            threshold=PROB_THRESHOLD,\n",
    "            draw_landmarks=has_landmarks,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame=frame,\n",
    "            message=\"Latency: {:.1f} ms\".format(latency),\n",
    "            position=(20, 30),\n",
    "            font_face=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale=FONT_SCALE,\n",
    "            color=PALETTE[0],\n",
    "            thickness=THICKNESS,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame=frame,\n",
    "            message=\"FPS: {:.1f}\".format(fps),\n",
    "            position=(20, 60),\n",
    "            font_face=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale=FONT_SCALE,\n",
    "            color=PALETTE[0],\n",
    "            thickness=THICKNESS,\n",
    "        )\n",
    "\n",
    "        framelist.append(frame)\n",
    "    return framelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-google",
   "metadata": {},
   "source": [
    "`get_results_for_model` ties everything together. It creates a pipeline for the specified model, runs inference, and creates a numpy array with results. It returns the resulting array and the FPS for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_for_model(modelname, input_filename, num_threads, num_streams, num_requests):\n",
    "    \"\"\"\n",
    "    Creates a pipeline for the specified model, runs inference, and creates a numpy array with results.\n",
    "    The function uses the `info` and `model_architectures` dictionaries that are created in previous notebook cells.\n",
    "\n",
    "    :param modelname: name of the model to use for inference, as given by Info Dumper\n",
    "    :param input_filename: input filename for video to run inference on\n",
    "    :param num_threads: number of threads for inference\n",
    "    :param num_streams: number of streams for inference\n",
    "    :param num_requests: maximum number of requests for inference\n",
    "    :return: list of frames with drawn detection results and the inference FPS\n",
    "    \"\"\"\n",
    "    # Create IE model\n",
    "    model_info = [item for item in info if item[\"name\"] == modelname][0]\n",
    "    model_xml = os.path.join(base_model_dir, model_info[\"subdirectory\"], PRECISION, modelname + \".xml\")\n",
    "    architecture_type = model_architectures[modelname]\n",
    "    ie = IECore()\n",
    "    model = get_model(ie=ie, model=Path(model_xml), architecture_type=architecture_type, labels=None)\n",
    "\n",
    "    # Create Async pipeline\n",
    "    plugin_config = {\n",
    "        \"CPU_THREADS_NUM\": f\"{num_threads}\",\n",
    "        \"CPU_THROUGHPUT_STREAMS\": f\"{num_streams}\",\n",
    "    }\n",
    "    detector_pipeline = AsyncPipeline(ie, model, plugin_config, device=DEVICE, max_num_requests=num_requests)\n",
    "\n",
    "    # Do inference\n",
    "    print(\n",
    "        f\"Starting inference. Model: {modelname}, video: {input_filename},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}\"\n",
    "    )\n",
    "    start_time = perf_counter()\n",
    "    inference_result = do_inference_on_video(\n",
    "        detector_pipeline=detector_pipeline, input_filename=input_filename, jump_frames=JUMP_FRAMES\n",
    "    )\n",
    "    end_time = perf_counter()\n",
    "\n",
    "    # Draw inference results on video frames and compute FPS\n",
    "    has_landmarks = architecture_type == \"retina\"\n",
    "    result_frames = make_result_frames(inference_result=inference_result, has_landmarks=has_landmarks)\n",
    "    fps = len(result_frames) / (end_time - start_time)\n",
    "\n",
    "    return result_frames, fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-browse",
   "metadata": {},
   "source": [
    "## Create widgets\n",
    "\n",
    "This demo works with a variety of [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models and allows you to use your own video.  We create widgets with [IPywidgets](https://github.com/jupyter-widgets/ipywidgets) to easily select a demo video or choose a video from your PC.\n",
    "\n",
    "Sample videos are downloaded from [Intel IoT Sample Videos](https://github.com/intel-iot-devkit/sample-videos). The link shows previews of the videos. If you upload your own video, it is recommended to use a short video. The maximum video size is 10MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-appreciation",
   "metadata": {},
   "source": [
    "## Download or upload a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-attitude",
   "metadata": {},
   "source": [
    "### Option 1: Download a sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video_base_url = \"https://github.com/intel-iot-devkit/sample-videos/raw/master\"\n",
    "sample_video_filenames = open(\"sample_videos.lst\").read().splitlines()\n",
    "sample_video_list = [(fn[:-4], f\"{sample_video_base_url}/{fn}\") for fn in sample_video_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = widgets.Dropdown(options=sample_video_list, index=5)\n",
    "sample_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-median",
   "metadata": {},
   "source": [
    "### Option 2: Upload your own video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "uploader = widgets.FileUpload(multiple=False)\n",
    "uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-bradley",
   "metadata": {},
   "source": [
    "`get_input_filename` checks if a video was uploaded. If so, it saves the uploaded video and returns the filename. If not, it downloads the sample video and returns the filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_filename():\n",
    "    \"\"\"\n",
    "    If a video is uploaded, process and save the uploaded video. If not, download the selected sample video.\n",
    "\n",
    "    :return: the filename of the uploaded video if available, or the filename of the selected sample video.\n",
    "    \"\"\"\n",
    "    if len(uploader.value) > 0:\n",
    "        uploaded_filename = next(iter(uploader.value))\n",
    "        if not os.path.exists(uploaded_filename):\n",
    "            content = uploader.value[uploaded_filename][\"content\"]\n",
    "            with open(uploaded_filename, \"wb\") as f:\n",
    "                f.write(content)\n",
    "        input_filename = uploaded_filename\n",
    "    else:\n",
    "        input_filename = os.path.basename(sample_video.value)\n",
    "        if not os.path.exists(input_filename):\n",
    "            download_video(sample_video.value)\n",
    "\n",
    "    return input_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-navigation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Detection results of one model, drawn on video\n",
    "\n",
    "Select a model and set the number of threads, streams and the maximum number of requests. ipywidgets\\* is used to make widgets for the model and option selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_inference = interact_manual.options(manual_name=\"Do inference\")\n",
    "\n",
    "\n",
    "@interact_inference(num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10))\n",
    "def show_results_on_model(\n",
    "    model=model_names,\n",
    "    num_threads=DEFAULT_NUM_THREADS,\n",
    "    num_streams=DEFAULT_NUM_STREAMS,\n",
    "    num_requests=DEFAULT_NUM_REQUESTS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform inference and display results for the selected model, with specified number of threads, streams and max number of requests.\n",
    "    NOTE: there is no error checking. Make sure that the selected model exists in IR format.\n",
    "    \"\"\"\n",
    "    input_filename = get_input_filename()\n",
    "    resultvideo, fps = get_results_for_model(model, input_filename, num_threads, num_streams, num_requests)\n",
    "    for item in resultvideo:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(item)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    print(\n",
    "        f\"Finished inference. Model: {model},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}. FPS: {fps:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-techno",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Detection results of multiple models\n",
    "\n",
    "Perform inference on selected models and show results on three random frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a widget to select multiple models. By default three face detection models are selected.\n",
    "select_model_widget = widgets.SelectMultiple(\n",
    "    description=\"Models\",\n",
    "    options=model_names,\n",
    "    index=[2, 5, 7],\n",
    "    layout=Layout(display=\"flex\", flex_flow=\"column\"),\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_inference(modelnames=select_model_widget, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10))\n",
    "def show_inference_multiple_models(\n",
    "    modelnames, num_threads=DEFAULT_NUM_THREADS, num_streams=DEFAULT_NUM_STREAMS, num_requests=DEFAULT_NUM_REQUESTS\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform inference for selected models and show results on three random frames of the input video.\n",
    "    \"\"\"\n",
    "    inference_results_multiple_models = []\n",
    "    input_filename = get_input_filename()\n",
    "    for i, modelname in enumerate(modelnames):\n",
    "        resultvideo, fps = get_results_for_model(modelname, input_filename, num_threads, num_streams, num_requests)\n",
    "        inference_results_multiple_models.append(resultvideo)\n",
    "        print(f\"--- Finished: FPS: {fps:.2f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(3, len(inference_results_multiple_models), figsize=(18, 12), squeeze=False)\n",
    "    indices = random.choices(range(len(inference_results_multiple_models[0])), k=3)\n",
    "    for i, resultvideo in enumerate(inference_results_multiple_models):\n",
    "        modelname = select_model_widget.value[i]\n",
    "        ax[0, i].set_title(modelname)\n",
    "        for j, framenr in enumerate(indices):\n",
    "            ax[j, i].imshow(resultvideo[framenr])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
