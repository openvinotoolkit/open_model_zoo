/*
// Copyright (C) 2018-2020 Intel Corporation
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
*/

#include "async_pipeline.h"
#include <samples/args_helper.hpp>
#include <cldnn/cldnn_config.hpp>
#include <samples/slog.hpp>

using namespace InferenceEngine;
    
PipelineBase::PipelineBase(std::unique_ptr<ModelBase> modelInstance, const CnnConfig& cnnConfig, InferenceEngine::Core* engine):
    model(std::move(modelInstance)){

    // --------------------------- 1. Load inference engine ------------------------------------------------
    slog::info << "Loading Inference Engine" << slog::endl;

    auto ie = engine ?
        std::unique_ptr<InferenceEngine::Core, void(*)(InferenceEngine::Core*)>(engine, [](InferenceEngine::Core*) {}) :
        std::unique_ptr<InferenceEngine::Core, void(*)(InferenceEngine::Core*)>(new InferenceEngine::Core, [](InferenceEngine::Core* ptr) {delete ptr; });

    slog::info << "Device info: " << slog::endl;
    slog::info<< ie->GetVersions(cnnConfig.devices);

    /** Load extensions for the plugin **/
    if (!cnnConfig.cpuExtensionsPath.empty()) {
        // CPU(MKLDNN) extensions are loaded as a shared library and passed as a pointer to base extension
        IExtensionPtr extension_ptr = make_so_pointer<IExtension>(cnnConfig.cpuExtensionsPath.c_str());
        ie->AddExtension(extension_ptr, "CPU");
    }
    if (!cnnConfig.clKernelsConfigPath.empty()) {
        // clDNN Extensions are loaded from an .xml description and OpenCL kernel files
        ie->SetConfig({ {PluginConfigParams::KEY_CONFIG_FILE, cnnConfig.clKernelsConfigPath} }, "GPU");
    }

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    slog::info << "Loading network files" << slog::endl;
    /** Read network model **/
    InferenceEngine::CNNNetwork cnnNetwork = ie->ReadNetwork(model->getModelFileName());
    /** Set batch size to 1 **/
    slog::info << "Batch size is forced to 1." << slog::endl;

    auto shapes = cnnNetwork.getInputShapes();
    for (auto& shape : shapes) {
        shape.second[0] = 1;
    }
    cnnNetwork.reshape(shapes);

    // -------------------------- Reading all outputs names and customizing I/O blobs (in inherited classes)
    model->prepareInputsOutputs(cnnNetwork);

    // --------------------------- 4. Loading model to the device ------------------------------------------
    slog::info << "Loading model to the device" << slog::endl;
    execNetwork = ie->LoadNetwork(cnnNetwork, cnnConfig.devices, cnnConfig.execNetworkConfig);

    // --------------------------- 5. Create infer requests ------------------------------------------------
    requestsPool.reset(new RequestsPool(execNetwork, cnnConfig.maxAsyncRequests));

    // --------------------------- 6. Call onLoadCompleted to complete initialization of model -------------
    model->onLoadCompleted(&execNetwork, requestsPool.get());
}

PipelineBase::~PipelineBase(){
    waitForTotalCompletion();
}

void PipelineBase::waitForData(){
    std::unique_lock<std::mutex> lock(mtx);

    condVar.wait(lock, [&] {return callbackException != nullptr ||
        requestsPool->isIdleRequestAvailable() ||
        completedInferenceResults.find(outputFrameId) != completedInferenceResults.end();
    });

    if (callbackException)
        std::rethrow_exception(callbackException);
}

int64_t PipelineBase::submitRequest(const InferenceEngine::InferRequest::Ptr& request, const std::shared_ptr<MetaData>& metaData){
    perfInfo.numRequestsInUse = (uint32_t)requestsPool->getInUseRequestsCount();

    auto frameStartTime = std::chrono::steady_clock::now();

    if (!perfInfo.startTime.time_since_epoch().count())
    {
        perfInfo.startTime = frameStartTime;
    }

    auto frameID = inputFrameId;

    request->SetCompletionCallback([this,
        frameStartTime,
        frameID,
        request,
        metaData] {
            {
                std::lock_guard<std::mutex> lock(mtx);

                try {
                    InferenceResult result;

                    result.startTime = frameStartTime;
                    perfInfo.lastInferenceLatency = std::chrono::steady_clock::now() - frameStartTime;

                    result.frameId = frameID;
                    result.metaData = std::move(metaData);
                    for (std::string outName : model->getOutputsNames())
                        result.outputsData.emplace(outName, std::make_shared<TBlob<float>>(*as<TBlob<float>>(request->GetBlob(outName))));

                    completedInferenceResults.emplace(frameID, result);
                    this->requestsPool->setRequestIdle(request);

                    this->onProcessingCompleted(request);
                }
                catch (...) {
                    if (!this->callbackException) {
                        this->callbackException = std::move(std::current_exception());
                    }
                }
            }
            condVar.notify_one();
    });

    inputFrameId++;
    if (inputFrameId < 0)
        inputFrameId = 0;

    request->StartAsync();
    return frameID;
}

int64_t PipelineBase::submitImage(cv::Mat img) {
    auto request = requestsPool->getIdleRequest();
    if (!request)
        return -1;

    MetaData* md;
    model->preprocess(ImageInputData(img), request, md);

    return submitRequest(request, std::shared_ptr<MetaData>(md));
}

std::unique_ptr<ResultBase> PipelineBase::getResult()
{
    auto infResult = PipelineBase::getInferenceResult();
    if (infResult.IsEmpty()) {
        return std::unique_ptr<ResultBase>();
    }

    auto result = model->postprocess(infResult);

    *result.get() = static_cast<ResultBase&>(infResult);
    return result;
}

InferenceResult PipelineBase::getInferenceResult()
{
    std::lock_guard<std::mutex> lock(mtx);

    const auto& it = completedInferenceResults.find(outputFrameId);

    if (it != completedInferenceResults.end())
    {
        auto retVal = std::move(it->second);
        completedInferenceResults.erase(it);

        outputFrameId = retVal.frameId;
        outputFrameId++;
        if (outputFrameId < 0)
            outputFrameId = 0;

        // Updating performance info
        auto now = std::chrono::steady_clock::now();
        auto latency = now - retVal.startTime;

        auto oldLatency = perfInternals.latenciesMs[perfInternals.currentIndex];
        auto oldRetrievalTimestamp = perfInternals.retrievalTimestamps[perfInternals.currentIndex];

        perfInternals.latenciesMs[perfInternals.currentIndex] = std::chrono::duration_cast<std::chrono::milliseconds>(latency).count();
        perfInternals.movingLatenciesSumMs += perfInternals.latenciesMs[perfInternals.currentIndex];
        perfInternals.retrievalTimestamps[perfInternals.currentIndex] = now;
        perfInternals.currentIndex = (perfInternals.currentIndex + 1) % MOVING_AVERAGE_SAMPLES;

        perfInfo.latencySum += latency;
        perfInfo.framesCount++;
        perfInfo.FPS = perfInfo.framesCount*1000.0 / std::chrono::duration_cast<std::chrono::milliseconds>(now - perfInfo.startTime).count();

        if (perfInfo.framesCount <= MOVING_AVERAGE_SAMPLES) {
            // History buffer is not full in the beginning, so let's reuse global metrics so far
            perfInfo.movingAverageLatencyMs = ((double)perfInternals.movingLatenciesSumMs) / perfInfo.framesCount;
            perfInfo.movingAverageFPS = perfInfo.FPS;
        }
        else
        {
            // Now history buffer is full, we can use its data
            perfInternals.movingLatenciesSumMs -= oldLatency;
            perfInfo.movingAverageLatencyMs = ((double)perfInternals.movingLatenciesSumMs) / MOVING_AVERAGE_SAMPLES;
            perfInfo.movingAverageFPS = MOVING_AVERAGE_SAMPLES * 1000.0 /
                std::chrono::duration_cast<std::chrono::milliseconds>(now - oldRetrievalTimestamp).count();
        }

        return retVal;
    }

    return InferenceResult();
}
